# =============================================================================
# âœ… BEST PRACTICE: MINIMAL + BULLETPROOF (Works EVERYWHERE)
# =============================================================================

from trl import SFTTrainer
from transformers import TrainingArguments

# ðŸ”¥ INDUSTRY STANDARD CONFIG (T4/A100/RTX ready)
training_args = TrainingArguments(
    output_dir="./imdb-qlora",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,    # Effective batch=8
    warmup_steps=10,
    max_steps=60,                     # 5-7 min demo
    learning_rate=2e-4,
    fp16=not torch.cuda.is_bf16_supported(),  # Auto-detect
    bf16=torch.cuda.is_bf16_supported(),      # A100 support
    logging_steps=5,
    save_steps=30,
    report_to="none",                 # No wandb
    # NO evaluation_strategy = NO ERRORS
)

# ðŸ”¥ CRITICAL: Unsloth training mode
model = FastLanguageModel.for_training(model)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset['train'],
    args=training_args,
    dataset_text_field="text",
    max_seq_length=512,
)

print("âœ… STANDARD QLoRA READY!")
print("ðŸš€ RUN: trainer.train()")
