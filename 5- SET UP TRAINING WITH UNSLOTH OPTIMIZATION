# =============================================================================
# STEP 5: SET UP TRAINING WITH UNSLOTH OPTIMIZATION
# =============================================================================
from trl import SFTTrainer
from transformers import TrainingArguments

# Configure training (T4 GPU optimized)
training_args = TrainingArguments(
    output_dir="./imdb-qlora",        # Where to save model
    per_device_train_batch_size=2,     # T4 can handle 2 with 4-bit
    gradient_accumulation_steps=4,      # Simulates batch size of 8
    warmup_steps=10,                    # Quick warmup for small dataset
    max_steps=60,                        # Only 60 steps for demo (~5 min)
    learning_rate=2e-4,                  # Standard for QLoRA
    fp16=True,                           # Use fp16 on T4
    logging_steps=5,                      # Show loss every 5 steps
    save_strategy="steps",                
    save_steps=30,                         # Save checkpoint every 30 steps
    evaluation_strategy="steps",           # Evaluate during training
    eval_steps=30,                          # Check test set every 30 steps
    report_to="none",                       # Don't use wandb/tensorboard
)

# Apply Unsloth training optimization FIRST ðŸ”¥
model = FastLanguageModel.for_training(model)

# Create trainer with validation
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],        # Now using test set for validation
    args=training_args,
    dataset_text_field="text",
    max_seq_length=512,
)

print("âœ… STEP 5 COMPLETE!")
print("ðŸš€ Ready to train! Run: trainer.train()")
