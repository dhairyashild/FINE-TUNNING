# TRANSFORMER: Input → Token Embeddings (word meanings) + Positional Embeddings (word positions) → Multi-Head Attention → Feed-Forward → Output
Input: "The cat sat on the mat"  ✅
     ↓
Static Embed: [0.2,0.5,..] [0.1,0.8,..] [0.4,0.3,..] [0.7,..] [0.2,..]
     ↓ + 
Positional:   [+sin(0),..] [+sin(1),..] [+sin(2),..] [+sin(3),..] [+sin(4),..]
     ↓
Attention: "sat" focuses 60% "cat"(subject) + 30% "mat"(object) + 10% others
     ↓
Output: Rich context vectors → FeedForward → LayerNorm → Next layers

############################################################################################################################



1- Word Embedding: Words → numbers capturing meaning/similarity | dim=300
                Extra: "king" - "man" + "woman" = "queen" (vector math)
# Word Embedding Representation (Simplified Vector Space)
word_matrix = {
    "Concept":  ["Battle", "Horse", "King", "Man", "Queen", "Woman", "Authority"],
    "Event":    [1.00,      0.00,    0.00,   0.00,  0.00,    0.00,    0.20],
    "Has Tail": [0.00,      1.00,    0.00,   0.10,  0.00,    0.00,    0.00],
    "Rich":     [0.00,      0.00,    1.00,   0.20,  1.00,    0.30,    0.20],
    "Gender":   [0.00,      0.00,    1.00,   1.00, -1.00,   -1.00,    0.00]
}
# Note: In real embeddings (like Word2Vec), 'Gender' is often represented 
# on a scale (e.g., 1 for Male, -1 for Female).

-----------------------------------------------------------------------------
Contextual Embedding: Dynamic word vectors change by sentence context |  (SAME WORD BUT DIFFERENT MEAN AS PER WHERE IT USED)
Extra: "bank" (river) ≠ "bank" (money) → same word, different vectors
-----------------------------------------------------------------------------
# TRANSFORMER ENCODER: A neural network component that processes input data using self-attention to understand context and relationships between elements in a sequence.= BERT HAVE ONLY ENCODER

# DECODER: A neural network that creates output sequences by looking at encoder outputs and focusing on its own previous Tokens.EG= GPT HAVE DECODER
-----------------------------------------------------------------------------

1= STATIC EMBEDING OF TOEKN ---> POSITIONAL EMBEDING BY ADDING SMALL VECTOR INTO MAIN VECTOR SO WORD POSITION DECIDED  BY THIS WHICH WORD 1ST , 2ND ...

DOSA , DHOKLA , --- ATTENDING WORD INDIAN IN THAT PAR

Q= QUERRY == I WANY QUANTUM COMPUTING BOOK
K= KEY= RACK IN LIB LIKE ENGINEERING SECTION MADE FIND KAR
V= VALUE = BOOK 

# Q (Query): Current word asking "what matters?"; K (Key): All words' labels; V (Value): Actual word info; Q•K scores decide which V to focus on.
# EXAMPLE: In "The cat sat" - For word "sat", Q looks at K of "cat" (label: animal) and gives high score to V of "cat", understanding what sat.

SOFTMAX CONVERT Q * K = INTO PROBABILITY OF WORD

# ATTENTION(Q,K,V): Mechanism where each word asks (Q) others, matches with labels (K), and pulls info (V) from relevant words.
-----------------------------------------------------------------------------



-----------------------------------------------------------------------------

SEE 5,6,7 VIDEO OF 3 BLUE AND 1 BROWN



-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------






-----------------------------------------------------------------------------





-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------




-----------------------------------------------------------------------------





-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------






-----------------------------------------------------------------------------





-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------








-----------------------------------------------------------------------------





-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------





-----------------------------------------------------------------------------





-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------





-----------------------------------------------------------------------------





-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------




-----------------------------------------------------------------------------





-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------





-----------------------------------------------------------------------------





-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------




-----------------------------------------------------------------------------





-----------------------------------------------------------------------------




-----------------------------------------------------------------------------






-----------------------------------------------------------------------------





